{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 232\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: train, normal images: 2286, pneumonia images: 1046\n",
      "Set: val, normal images: 328, pneumonia images: 153\n",
      "Set: test, normal images: 639, pneumonia images: 296\n"
     ]
    }
   ],
   "source": [
    "input_path = 'C:/Users/shahe/Desktop/Knee Osteoarthritis/'\n",
    "for _set in ['train', 'val', 'test']:\n",
    "    n_0 = len(os.listdir(input_path + _set + '/0'))\n",
    "    n_1 = len(os.listdir(input_path + _set + '/1'))\n",
    "    n_2 = len(os.listdir(input_path + _set + '/2'))\n",
    "    n_3 = len(os.listdir(input_path + _set + '/3'))\n",
    "    n_4 = len(os.listdir(input_path + _set + '/4'))\n",
    "    print('Set: {}, normal images: {}, pneumonia images: {}'.format(_set, n_0, n_1, n_2, n_3, n_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(img_dims, batch_size):\n",
    "    # Data generation objects\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, vertical_flip=True)\n",
    "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # This is fed to the network in the specified batch sizes and image dimensions\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "    directory=input_path+'train', \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True)\n",
    "\n",
    "    test_gen = test_val_datagen.flow_from_directory(\n",
    "    directory=input_path+'test', \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True)\n",
    "    \n",
    "    # I will be making predictions off of the test set in one batch size\n",
    "    # This is useful to be able to get the confusion matrix\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    for cond in ['/0/', '/1/', '/2/', '/3/', '/4/']:\n",
    "        for img in (os.listdir(input_path + 'test' + cond)):\n",
    "            img = plt.imread(input_path+'test'+cond+img)\n",
    "            img = cv2.resize(img, (img_dims, img_dims))\n",
    "            img = np.dstack([img, img, img])\n",
    "            img = img.astype('float32') / 255\n",
    "            if cond=='/0/':\n",
    "                label = 0\n",
    "            elif cond=='/1/':\n",
    "                label = 1\n",
    "            elif cond=='/2/':\n",
    "                label = 2\n",
    "            elif cond=='/3/':\n",
    "                label = 3\n",
    "            elif cond=='/4/':\n",
    "                label = 4\n",
    "            test_data.append(img)\n",
    "            test_labels.append(label)\n",
    "        \n",
    "    test_data = np.array(test_data)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_gen, test_gen, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5778 images belonging to 5 classes.\n",
      "Found 1656 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "img_dims = 150\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(img_dims, img_dims, 3))\n",
    "\n",
    "# First conv block\n",
    "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Second conv block\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Third conv block\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Fourth conv block\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "# Fifth conv block\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "# FC layer\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dropout(rate=0.7)(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "x = Dense(units=64, activation='relu')(x)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(units=5, activation='sigmoid')(x)\n",
    "\n",
    "# Creating model and compiling\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='KneeOsteoModel.h5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 343s 2s/step - loss: 0.5306 - accuracy: 0.3301 - val_loss: 0.4568 - val_accuracy: 0.3866\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 279s 2s/step - loss: 0.4693 - accuracy: 0.3667 - val_loss: 0.4509 - val_accuracy: 0.3848\n",
      "Epoch 3/10\n",
      "171/180 [===========================>..] - ETA: 12s - loss: 0.4622 - accuracy: 0.3873"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
    "           epochs=epochs, validation_data=test_gen, \n",
    "           validation_steps=test_gen.samples // batch_size, callbacks=[checkpoint, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
