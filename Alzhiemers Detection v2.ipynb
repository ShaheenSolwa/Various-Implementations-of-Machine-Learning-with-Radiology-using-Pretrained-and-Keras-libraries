{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 232\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: train, normal images: 717, pneumonia images: 52\n",
      "Set: test, normal images: 179, pneumonia images: 12\n"
     ]
    }
   ],
   "source": [
    "input_path = 'C:/Users/shahe/Desktop/alzheimers/'\n",
    "#'MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented'\n",
    "for _set in ['train', 'test']:\n",
    "    n_mild = len(os.listdir(input_path + _set + '/MildDemented'))\n",
    "    n_moderate = len(os.listdir(input_path + _set + '/ModerateDemented'))\n",
    "    n_non = len(os.listdir(input_path + _set + '/NonDemented'))\n",
    "    n_veryMild = len(os.listdir(input_path + _set + '/VeryMildDemented'))\n",
    "    print('Set: {}, normal images: {}, pneumonia images: {}'.format(_set,n_mild, n_moderate, n_non, n_veryMild))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(img_dims, batch_size):\n",
    "    # Data generation objects\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, vertical_flip=True)\n",
    "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # This is fed to the network in the specified batch sizes and image dimensions\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "    directory=input_path+'train', \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True)\n",
    "\n",
    "    test_gen = test_val_datagen.flow_from_directory(\n",
    "    directory=input_path+'test', \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical', \n",
    "    shuffle=True)\n",
    "    \n",
    "    # I will be making predictions off of the test set in one batch size\n",
    "    # This is useful to be able to get the confusion matrix\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    #'MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented'\n",
    "    for cond in ['/MildDemented/', '/ModerateDemented/', '/NonDemented/', '/VeryMildDemented/']:\n",
    "        for img in (os.listdir(input_path + 'test' + cond)):\n",
    "            img = plt.imread(input_path+'test'+cond+img)\n",
    "            img = cv2.resize(img, (img_dims, img_dims))\n",
    "            img = np.dstack([img, img, img])\n",
    "            img = img.astype('float32') / 255\n",
    "            if cond=='/MildDemented/':\n",
    "                label = 0\n",
    "            elif cond=='/ModerateDemented/':\n",
    "                label = 1\n",
    "            elif cond=='/NonDemented/':\n",
    "                label = 2\n",
    "            elif cond=='/VeryMildDemented/':\n",
    "                label = 3\n",
    "            test_data.append(img)\n",
    "            test_labels.append(label)\n",
    "        \n",
    "    test_data = np.array(test_data)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_gen, test_gen, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5121 images belonging to 4 classes.\n",
      "Found 1279 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "img_dims = 150\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(img_dims, img_dims, 3))\n",
    "\n",
    "# First conv block\n",
    "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Second conv block\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Third conv block\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Fourth conv block\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "# Fifth conv block\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "\n",
    "# FC layer\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dropout(rate=0.7)(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "x = Dense(units=64, activation='relu')(x)\n",
    "x = Dropout(rate=0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(units=4, activation='sigmoid')(x)\n",
    "\n",
    "# Creating model and compiling\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='Alzheimers_Weights.h5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=1, verbose=2, mode='auto')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 223s 1s/step - loss: 0.5310 - accuracy: 0.4555 - val_loss: 0.4759 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 241s 2s/step - loss: 0.4254 - accuracy: 0.5398 - val_loss: 0.4961 - val_accuracy: 0.4992\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 254s 2s/step - loss: 0.4123 - accuracy: 0.5574 - val_loss: 0.4631 - val_accuracy: 0.4992\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 270s 2s/step - loss: 0.4029 - accuracy: 0.5674 - val_loss: 0.5478 - val_accuracy: 0.5016\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 291s 2s/step - loss: 0.3995 - accuracy: 0.5654 - val_loss: 0.4163 - val_accuracy: 0.5224\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 257s 2s/step - loss: 0.4062 - accuracy: 0.5682 - val_loss: 0.4875 - val_accuracy: 0.5168\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 239s 1s/step - loss: 0.4029 - accuracy: 0.5667 - val_loss: 0.3986 - val_accuracy: 0.5737\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 257s 2s/step - loss: 0.3966 - accuracy: 0.5829 - val_loss: 0.3928 - val_accuracy: 0.5817\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 252s 2s/step - loss: 0.3956 - accuracy: 0.5699 - val_loss: 0.4019 - val_accuracy: 0.5705\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
      "Epoch 10/10\n",
      "112/160 [====================>.........] - ETA: 1:15 - loss: 0.3840 - accuracy: 0.5904"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
    "           epochs=epochs, validation_data=test_gen, \n",
    "           validation_steps=test_gen.samples // batch_size, callbacks=[checkpoint, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
